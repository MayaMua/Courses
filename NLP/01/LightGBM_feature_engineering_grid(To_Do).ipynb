{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意(attention)！开始做前必读项！\n",
    "所有的代码一定要在这个文件里面编写，不要自己创建一个新的文件\n",
    "对于提供的数据集，不要改存储地方，也不要修改文件名和内容\n",
    "不要重新定义函数（如果我们已经定义好的），按照里面的思路来编写。当然，除了我们定义的部分，如有需要可以自行定义函数或者模块\n",
    "写完之后，重新看一下哪一部分比较慢，然后试图去优化。一个好的习惯是每写一部分就思考这部分代码的时间复杂度和空间复杂度，AI工程是的日常习惯！\n",
    "这次作业很重要，一定要完成！ 相信会有很多的收获！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入相关的包 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "# from sklearn.externals import joblib\n",
    "from bayes_opt import BayesianOptimization\n",
    "from gensim import models\n",
    "import joblib\n",
    "import re\n",
    "import jieba\n",
    "# from scipy.special import softmax\n",
    "from sklearn.grid_search import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 500  # 表示样本表示最大的长度,表示降维之后的维度\n",
    "sentence_max_length = 1500  # 表示句子/样本在降维之前的维度\n",
    "Train_features3, Test_features3, Train_label3, Test_label3 = [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取训练好的词嵌入向量和文本文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeyedVectors.load_word2vec_format(file path, binary)  \n",
    "file path: 词向量模型路径  \n",
    "binary: text format or binary format\n",
    "\n",
    "It turns out that load_word2vec_format is used when we're trying to load word vectors that are trained using the original implementation of word2vec (in C). Since these pre-trained word vectors are trained using Python (gensim), we can use load instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load w2v_embedding model\n",
      "load fast_embedding model\n",
      "fast_embedding输出词表的个数116571,w2v_embedding输出词表的个数19592\n",
      "取词向量成功\n"
     ]
    }
   ],
   "source": [
    "# ToDo\n",
    "# 通过models.KeyedVectors加载预训练好的embedding\n",
    "# fast_embedding = models.KeyedVectors.load_word2vec_format(\"fast.model\", binary=False, unicode_errors='ignore')\n",
    "# w2v_embedding = models.KeyedVectors.load_word2vec_format(\"w2v.model\", binary=False, unicode_errors='ignore')\n",
    "print('load w2v_embedding model')\n",
    "w2v_embedding =  models.Word2Vec.load('w2v.model')  # Todo\n",
    "print('load fast_embedding model')\n",
    "fast_embedding = models.FastText.load('fast.model')  # Todo\n",
    "print(\"fast_embedding输出词表的个数{},w2v_embedding输出词表的个数{}\".format(\n",
    "    len(fast_embedding.wv.vocab.keys()), len(w2v_embedding.wv.vocab.keys())))\n",
    "\n",
    "print(\"取词向量成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取数据完成\n"
     ]
    }
   ],
   "source": [
    "# ToDo\n",
    "# 读取train_clean.tsv ，test_clean.tsv训练集和测试集文件\n",
    "# hint: 通过pandas中的read_csv读取数据集\n",
    "train = pd.read_csv('train_clean.csv', sep='\\t')\n",
    "test = pd.read_csv('test_clean.csv', sep='\\t')\n",
    "\n",
    "\n",
    "# train = train[0:1000]\n",
    "# test = test[0:1000]\n",
    "\n",
    "print(\"读取数据完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['文化', '文学', '管理', '社会科学', '小说', '大中专教材教辅', '艺术', '外语学习', '建筑',\n",
       "       '旅游/地图', '励志与成功', '童书', '医学', '科学与自然', '工业技术', '中小学教辅', '历史', '考试',\n",
       "       '青春文学', '哲学/宗教', '法律', '计算机与互联网', '育儿/家教', '金融与投资', '政治/军事',\n",
       "       '健身与保健', '孕产/胎教', '农业/林业', '国学/古籍', '动漫', '烹饪/美食', '科普读物', '婚恋与两性'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.664 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['文化'],\n",
       " ['文学'],\n",
       " ['管理'],\n",
       " ['社会科学'],\n",
       " ['小说'],\n",
       " ['大中专', '教材', '教辅'],\n",
       " ['艺术'],\n",
       " ['外语', '学习'],\n",
       " ['建筑'],\n",
       " ['旅游', '/', '地图'],\n",
       " ['励志', '与', '成功'],\n",
       " ['童书'],\n",
       " ['医学'],\n",
       " ['科学', '与', '自然'],\n",
       " ['工业', '技术'],\n",
       " ['中小学', '教辅'],\n",
       " ['历史'],\n",
       " ['考试'],\n",
       " ['青春文学'],\n",
       " ['哲学', '/', '宗教'],\n",
       " ['法律'],\n",
       " ['计算机', '与', '互联网'],\n",
       " ['育儿', '/', '家教'],\n",
       " ['金融', '与', '投资'],\n",
       " ['政治', '/', '军事'],\n",
       " ['健身', '与', '保健'],\n",
       " ['孕产', '/', '胎教'],\n",
       " ['农业', '/', '林业'],\n",
       " ['国学', '/', '古籍'],\n",
       " ['动漫'],\n",
       " ['烹饪', '/', '美食'],\n",
       " ['科普读物'],\n",
       " ['婚恋', '与', '两性']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = []\n",
    "for l in train.label.unique():\n",
    "    l_ = list(jieba.cut(l, cut_all=False))\n",
    "    label_list.append(l_)\n",
    "\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useless_words = ['/', '与']\n",
    "for l in label_list:\n",
    "    for w in useless_words:\n",
    "        if w in l:\n",
    "            l.remove(w)\n",
    "len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['文化'],\n",
       " ['文学'],\n",
       " ['管理'],\n",
       " ['社会科学'],\n",
       " ['小说'],\n",
       " ['大中专', '教材', '教辅'],\n",
       " ['艺术'],\n",
       " ['外语', '学习'],\n",
       " ['建筑'],\n",
       " ['旅游', '地图'],\n",
       " ['励志', '成功'],\n",
       " ['童书'],\n",
       " ['医学'],\n",
       " ['科学', '自然'],\n",
       " ['工业', '技术'],\n",
       " ['中小学', '教辅'],\n",
       " ['历史'],\n",
       " ['考试'],\n",
       " ['青春文学'],\n",
       " ['哲学', '宗教'],\n",
       " ['法律'],\n",
       " ['计算机', '互联网'],\n",
       " ['育儿', '家教'],\n",
       " ['金融', '投资'],\n",
       " ['政治', '军事'],\n",
       " ['健身', '保健'],\n",
       " ['孕产', '胎教'],\n",
       " ['农业', '林业'],\n",
       " ['国学', '古籍'],\n",
       " ['动漫'],\n",
       " ['烹饪', '美食'],\n",
       " ['科普读物'],\n",
       " ['婚恋', '两性']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/home/jwu51/.conda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_embedding = []\n",
    "\n",
    "for l in label_list:\n",
    "    label_vector = []\n",
    "    for w in l:\n",
    "        label_vector.append(fast_embedding[w])\n",
    "    label_embedding.append(list(np.mean(label_vector, axis=0)))\n",
    "\n",
    "label_embedding = np.asarray(label_embedding)\n",
    "label_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将df中的label映射为数字标签并保存到labelIndex列中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelName = train.label.unique()  # 全部label列表\n",
    "labelIndex = list(range(len(labelName)))  # 全部label标签\n",
    "labelNameToIndex = dict(zip(labelName, labelIndex))  # label的名字对应标签的字典\n",
    "labelIndexToName = dict(zip(labelIndex, labelName))  # label的标签对应名字的字典\n",
    "train[\"labelIndex\"] = train.label.map(labelNameToIndex)\n",
    "test[\"labelIndex\"] = test.label.map(labelNameToIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "切分数据完成\n"
     ]
    }
   ],
   "source": [
    "def query_cut(query):\n",
    "    '''\n",
    "    函数说明：该函数用于对输入的语句（query）按照空格进行切分\n",
    "    '''\n",
    "    # ToDo\n",
    "    # 第一步：定义一个query_cut函数 将query按空格划分并返回，\n",
    "    # hint: 通过sqlit对query进行划分\n",
    "    query_list = query.split(\" \")\n",
    "    return query_list\n",
    "\n",
    "# 第二步：然后train和test中的每一个样本都调用该函数，\n",
    "#      并将划分好的样本分别存储到train[\"queryCut\"]和test[\"queryCut\"]中\n",
    "\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for i in range(len(list(train.text))):\n",
    "    train_list.append(query_cut(train.text[i]))\n",
    "    \n",
    "for i in range(len(test)):\n",
    "    test_list.append(query_cut(test.text[i]))    \n",
    "    \n",
    "train[\"queryCut\"] = train_list\n",
    "test[\"queryCut\"] = test_list\n",
    "print(\"切分数据完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt', \"r\", encoding=\"utf8\") as f:\n",
    "    #ToDo\n",
    "    # 第一步：按行读取停用词文件\n",
    "    stopWords = [line.rstrip() for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除停用词\n"
     ]
    }
   ],
   "source": [
    "def rm_stop_word(wordList):\n",
    "    '''\n",
    "    函数说明：该函数用于去除输入样本中的存在的停用词\n",
    "    Return: 返回去除停用词之后的样本\n",
    "    '''\n",
    "    # ToDo\n",
    "    # 第二步：去除每个样本中的停用词并返回新的样本\n",
    "    return [w for w in wordList if w not in stopWords and not re.match('^[a-z|A-Z|0-9|.]*$',w)]\n",
    "    \n",
    "train[\"queryCutRMStopWord\"] = train[\"queryCut\"].apply(rm_stop_word)\n",
    "# dev[\"queryCutRMStopWord\"] = dev[\"text\"].apply(rm_stop_word)\n",
    "test[\"queryCutRMStopWord\"] = test[\"queryCut\"].apply(rm_stop_word)\n",
    "print(\"去除停用词\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [杏林, 芳菲, 广东, 中医药, 曹磊, 编著, 杏林, 芳菲, 广东, 中医药, 注重,...\n",
       "1         [额吉, 白云, 优秀, 蒙古文, 文学作品, 翻译, 出版, 工程, 第五辑, 散文, 卷...\n",
       "2         [叶赛宁, 抒情诗, 选, 本书, 收入, 叶赛宁, 首, 抒情诗, 多为, 名篇, 佳作,...\n",
       "3         [长发, 腰, 娶, 可好, 走过, 唐诗, 路过, 宋词, 相约, 江南, 美文, 佳景,...\n",
       "4         [公司债务, 融资, 相关, 金融, 系统, 本书, 经济, 生活, 热点, 前沿, 公司债...\n",
       "                                ...                        \n",
       "206311    [四个, 学习, 读本, 首次, 权威, 定义, 五论, 协调, 推进, 四个, 评论员, ...\n",
       "206312    [湖北省, 湖泊, 志, 下册, 全, 本书, 收录, 湖北省, 中型, 湖泊, 小型, 湖...\n",
       "206313    [年, 计算机专业, 基础, 综合, 考试, 真题, 思路, 分析, 本书, 计算机专业, ...\n",
       "206314    [野生, 作家, 访谈录, 写作, 现场, 本书, 收录, 文章, 界面, 文化频道, 记者...\n",
       "206315    [西洋, 政治, 思想史, 中世纪, 篇, 本书, 欧洲, 日耳曼人, 政治, 思想, 萌发...\n",
       "Name: queryCutRMStopWord, Length: 206316, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"queryCutRMStopWord\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def Find_embedding_with_windows(embedding_matrix):\n",
    "    '''\n",
    "    函数说明：该函数用于获取在大小不同的滑动窗口(k=[2, 3, 4])， 然后进行平均或取最大操作。\n",
    "    参数说明：\n",
    "        - embedding_matrix：样本中所有词构成的词向量矩阵\n",
    "    return: result_list 返回拼接而成的一维词向量\n",
    "    '''\n",
    "        \n",
    "    '''\n",
    "    embedding_matrix\n",
    "    \n",
    "    [[0, 0, 0, 0, 0],\n",
    "     [0, 0, 0, 0, 0],\n",
    "     [0, 0, 0, 0, 0]]\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    result_list = []\n",
    "    \n",
    "    size = [2, 3, 4]\n",
    "    k = random.choice([2, 3, 4])\n",
    "    op = random.choice([1, 2])  # 1:ave, 2: max\n",
    "    # ToDo:\n",
    "    # 由于之前抽取的特征并没有考虑词与词之间交互对模型的影响，\n",
    "    # 对于分类模型来说，贡献最大的不一定是整个句子， 可能是句子中的一部分， 如短语、词组等等。 \n",
    "    # 在此基础上我们使用大小不同的滑动窗口(k=[2, 3, 4])， 然后进行平均或取最大操作。\n",
    "    \n",
    "    \n",
    "    for k in size:\n",
    "        a = np.zeros(shape = embedding_matrix.shape)\n",
    "        for i in range(embedding_matrix.shape[0] - k + 1):\n",
    "            a[i] = embedding_matrix[i:i+k].mean(axis=0)\n",
    "        result_list.extend(a.max(axis=0).tolist())\n",
    "        \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Label_embedding(word_matrix, label_embedding):\n",
    "    '''\n",
    "    函数说明：获取到所有类别的 label embedding， 与输入的 word embedding 矩阵相乘， 对其结果进行 softmax 运算，\n",
    "            对 attention score 与输入的 word embedding 相乘的结果求平均或者取最大\n",
    "            可以参考论文《Joint embedding of words and labels》获取标签空间的词嵌入\n",
    "    parameters:\n",
    "    -- example_matrix(np.array 2D): denotes the matrix of words embedding\n",
    "    -- embedding(np.array 2D): denotes the embedding of all label in data\n",
    "    return: (np.array 1D) the embedding by join label and word\n",
    "    '''\n",
    "    result_embedding=[]\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    比如一条数据的输入的是9*300(sequence length, embedding dim)，label的数量是33，得到的attention score为（9,1），\n",
    "    然后word embedding 和attention,进行np.multipy(word_embedding.T，attention )，得到（300,9）的输出，\n",
    "    然后再进行avg/max，得到(300,)特征向量，和别的特征进行拼接\n",
    "    '''\n",
    "    \n",
    "    # To_Do \n",
    "      # 第一步：基于consin相似度计算word embedding向量与label embedding之间的相似度\n",
    "    \n",
    "    matrix = np.dot(word_matrix, label_embedding.T)   #(9*33)\n",
    "    res_matrix = []\n",
    "    \n",
    "    # 第二步：通过softmax获取注意力分布    (9*1)\n",
    "    \n",
    "    k=3 # window size\n",
    "    \n",
    "    for i in range(matrix.shape[1] - k + 1):\n",
    "        tmp_matrix = matrix[:, i:i+k].mean(axis=1).tolist()\n",
    "#         print(tmp_matrix)\n",
    "        res_matrix.append(tmp_matrix)\n",
    "        \n",
    "    res_matrix = np.asarray(res_matrix)\n",
    "    res_matrix = res_matrix.max(axis=0)\n",
    "    attention = softmax(res_matrix)\n",
    "                          \n",
    "    # 第三步：将求得到的注意力分布与输入的word embedding相乘，并对结果进行最大化或求平均 (300, )\n",
    "    \n",
    "    result_embedding = (word_matrix.T * attention).T.max(axis=0)\n",
    "    \n",
    "    \n",
    "    return result_embedding\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(query):\n",
    "    '''\n",
    "    函数说明：联合多种特征工程来构造新的样本表示，主要通过以下三种特征工程方法\n",
    "            第一：利用word-embedding的average pooling和max-pooling\n",
    "            第二：利用窗口size=2，3，4对word-embedding进行卷积操作，然后再进行max/avg-pooling操作\n",
    "            第二：利用类别标签的表示，增加了词语和标签之间的语义交互，以此达到对词级别语义信息更深层次的考虑\n",
    "            另外，对于词向量超过预定义的长度则进行截断，小于则进行填充\n",
    "    参数说明：\n",
    "    - query:数据集中的每一个样本\n",
    "    return: 返回样本经过特征工程之后得到的词向量\n",
    "    '''\n",
    "    global max_length\n",
    "    arr = []\n",
    "    # 加载fast_embedding,w2v_embedding\n",
    "    global fast_embedding, w2v_embedding\n",
    "    fast_arr = np.array([fast_embedding.wv.get_vector(s)\n",
    "                         for s in query if s in fast_embedding.wv.vocab.keys()])\n",
    "    \n",
    "    # 在fast_arr下滑动获取到的词向量\n",
    "    if len(fast_arr) > 0:\n",
    "        windows_fastarr = np.array(Find_embedding_with_windows(fast_arr))\n",
    "        result_attention_embedding = Find_Label_embedding(\n",
    "            fast_arr, label_embedding)\n",
    "        \n",
    "        \n",
    "    else:# 如果样本中的词都不在字典，则该词向量初始化为0\n",
    "        # 这里300表示训练词嵌入设置的维度为300\n",
    "        windows_fastarr = np.zeros(300) \n",
    "        result_attention_embedding = np.zeros(300)\n",
    "\n",
    "    fast_arr_max = np.max(np.array(fast_arr), axis=0) if len(\n",
    "        fast_arr) > 0 else np.zeros(300)\n",
    "    fast_arr_avg = np.mean(np.array(fast_arr), axis=0) if len(\n",
    "        fast_arr) > 0 else np.zeros(300)\n",
    "\n",
    "    fast_arr = np.hstack((fast_arr_avg, fast_arr_max))\n",
    "    # 将多个embedding进行横向拼接\n",
    "    arr = np.hstack((np.hstack((fast_arr, windows_fastarr)),\n",
    "                     result_attention_embedding))\n",
    "    global sentence_max_length\n",
    "    # 如果样本的维度大于指定的长度则需要进行截取或者拼凑,\n",
    "#     result_arr = arr[:sentence_max_length] if len(arr) > sentence_max_length else np.hstack((\n",
    "#         arr, np.zeros(int(sentence_max_length-len(arr)))))\n",
    "    if len(arr) > sentence_max_length:\n",
    "        indice = random.sample(range(0, len(arr)), sentence_max_length)\n",
    "        indice.sort()\n",
    "        result_arr = arr[indice]\n",
    "        \n",
    "    else: \n",
    "        result_arr = np.hstack((arr, np.zeros(int(sentence_max_length-len(arr)))))\n",
    "    \n",
    "#     print(result_arr.shape)\n",
    "    \n",
    "    return result_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dimension_Reduction(Train, Test):\n",
    "    '''\n",
    "    函数说明：该函数通过PCA算法对样本进行降维，由于目前维度不是特别高 ，可以选择不降维。\n",
    "    参数说明：\n",
    "    - Train: 表示训练数据集\n",
    "    - Test: 表示测试数据集\n",
    "    Return: 返回降维之后的数据样本\n",
    "    '''\n",
    "    global max_length\n",
    "    #  To_Do \n",
    "    # 特征选择，由于经过特征工程得到的样本表示维度很高，因此需要进行降维 max_length表示降维之后的样本最大的维度。\n",
    "    # 这里通过PCA方法降维\n",
    "    \n",
    "    estimator = PCA(n_components=max_length)\n",
    "    pca_train = estimator.fit_transform(Train)\n",
    "    pca_test = estimator.fit_transform(Test)\n",
    "    return pca_train, pca_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Embedding():\n",
    "    '''\n",
    "    函数说明：该函数用于获取经过特征工程之后的样本表示\n",
    "    Return:训练集特征数组(2D)，测试集特征数组(2D)，训练集标签数组（1D）,测试集标签数组（1D）\n",
    "    '''\n",
    "    print(\"获取样本表示中...\")\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    Train_features2 = min_max_scaler.fit_transform(\n",
    "        np.vstack(train[\"queryCutRMStopWord\"].apply(sentence2vec)))\n",
    "    Test_features2 = min_max_scaler.fit_transform(\n",
    "        np.vstack(test[\"queryCutRMStopWord\"].apply(sentence2vec)))\n",
    "    print(\"获取样本词表示完成\")\n",
    "    # 通过PCA对样本表示进行降维\n",
    "    Train_features2, Test_features2 = Dimension_Reduction(\n",
    "        Train=Train_features2, Test=Test_features2)\n",
    "    Train_label2 = train[\"labelIndex\"]\n",
    "    Test_label2 = test[\"labelIndex\"]\n",
    "\n",
    "    print(\"加载训练好的词向量\")\n",
    "    print(\"Train_features.shape =\", Train_features2.shape)\n",
    "    print(\"Test_features.shape =\", Test_features2.shape)\n",
    "    print(\"Train_label.shape =\", Train_label2.shape)\n",
    "    print(\"Test_label.shape =\", Test_label2.shape)\n",
    "\n",
    "    return Train_features2, Test_features2, Train_label2, Test_label2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(Train_label, Test_label, Train_predict_label, Test_predict_label, model_name):\n",
    "    '''\n",
    "    函数说明：直接输出训练集和测试在模型上的准确率\n",
    "    参数说明：\n",
    "        - Train_label: 真实的训练集标签（1D）\n",
    "        - Test_labelb: 真实的测试集标签（1D）\n",
    "        - Train_predict_label: 模型在训练集上的预测的标签(1D)\n",
    "        - Test_predict_label: 模型在测试集上的预测标签（1D）\n",
    "        - model_name: 表示训练好的模型\n",
    "    Return: None\n",
    "    '''\n",
    "    # ToDo\n",
    "    \n",
    "    # 通过调用metrics.accuracy_score计算训练集和测试集上的准确率  Search_Flag+\n",
    "    print(model_name+'_'+'Train accuracy %s' % metrics.accuracy_score(Train_label, Train_predict_label))\n",
    "    # 输出测试集的准确率   Search_Flag+\n",
    "    print(model_name+'_'+'Test accuracy %s' %  metrics.accuracy_score(Test_label, Test_predict_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo\n",
    "第一步：完成LGBMClassifier模型中参数  \n",
    "第二步：根据joblib.dump保存训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grid_Train_model(Train_features, Test_features, Train_label, Test_label):\n",
    "    '''\n",
    "    函数说明：基于网格搜索优化的方法搜索模型最优参数，最后保存训练好的模型\n",
    "    参数说明：\n",
    "        - Train_features: 训练集特征数组（2D）\n",
    "        - Test_features: 测试集特征数组（2D）\n",
    "        - Train_label: 真实的训练集标签 (1D)\n",
    "        - Test_label: 真实的测试集标签（1D）\n",
    "    Return: None\n",
    "    '''\n",
    "#     parameters = {\n",
    "#         'max_depth': [5, 10, 15, 20, 25],\n",
    "#         'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15],\n",
    "#         'n_estimators': [100, 500, 1000, 1500, 2000],\n",
    "#         'min_child_weight': [0, 2, 5, 10, 20],\n",
    "#         'max_delta_step': [0, 0.2, 0.6, 1, 2],\n",
    "#         'subsample': [0.6, 0.7, 0.8, 0.85, 0.95],\n",
    "#         'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "#         'reg_alpha': [0, 0.25, 0.5, 0.75, 1],\n",
    "#         'reg_lambda': [0.2, 0.4, 0.6, 0.8, 1],\n",
    "#         'scale_pos_weight': [0.2, 0.4, 0.6, 0.8, 1]\n",
    "#     }\n",
    "    \n",
    "    \n",
    "    parameters = {\n",
    "        'max_depth': [1, 5],\n",
    "        'learning_rate': [0.01, 0.05],\n",
    "        'n_estimators': [100, 1000],\n",
    "        'min_child_weight': [0, 6],\n",
    "        'max_delta_step': [0, 0.6],\n",
    "        'subsample': [0.6, 0.75],\n",
    "#         'colsample_bytree': [0.5, 0.8],\n",
    "        'reg_alpha': [0, 0.5],\n",
    "#         'reg_lambda': [0.2, 1],\n",
    "#         'scale_pos_weight': [0.2, 0.8]\n",
    "    }\n",
    "    # 定义分类模型列表，这里仅使用LightGBM模型\n",
    "    models = [\n",
    "        lgb.LGBMClassifier(),\n",
    "    ]\n",
    "    # 遍历模型\n",
    "    for model in models:\n",
    "        model_name = model.__class__.  __name__\n",
    "        gsearch = GridSearchCV(\n",
    "            model, param_grid=parameters, scoring='accuracy', cv=3)\n",
    "        gsearch.fit(Train_features, Train_label)\n",
    "        # 输出最好的参数-\n",
    "        print(\"Best parameters set found on development set:{}\".format(\n",
    "            gsearch.best_params_))\n",
    "        Test_predict_label = gsearch.predict(Test_features)\n",
    "        Train_predict_label = gsearch.predict(Train_features)\n",
    "        Predict(Train_label, Test_label,\n",
    "                Train_predict_label, Test_predict_label, model_name)\n",
    "    # 保存训练好的模型\n",
    "    joblib.dump('#ToDo'+'.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主函数,先求训练集和测试集的词向量，然后根据Grid搜索来找到最佳参数的分类模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已完成特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取样本表示中...\n",
      "获取样本词表示完成\n",
      "加载训练好的词向量\n",
      "Train_features.shape = (206316, 500)\n",
      "Test_features.shape = (29474, 500)\n",
      "Train_label.shape = (206316,)\n",
      "Test_label.shape = (29474,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8aadee414546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFind_Embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mGrid_Train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTest_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTest_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrain_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTest_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-c08b0f0ac327>\u001b[0m in \u001b[0;36mGrid_Train_model\u001b[0;34m(Train_features, Test_features, Train_label, Test_label)\u001b[0m\n\u001b[1;32m     44\u001b[0m         gsearch = GridSearchCV(\n\u001b[1;32m     45\u001b[0m             model, param_grid=parameters, scoring='accuracy', cv=3)\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mgsearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;31m# 输出最好的参数-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         print(\"Best parameters set found on development set:{}\".format(\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    713\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 715\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    798\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    593\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1924\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1925\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1926\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1927\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Train_features, Test_features, Train_label, Test_label = Find_Embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "没训练出来 不再调试模型了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid_Train_model(Train_features=Train_features, Test_features=Test_features,Train_label=Train_label, Test_label=Test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_x_1 = open('Train_features.pickle', 'wb')\n",
    "pickle.dump(Train_features, file_x_1)\n",
    "file_x_1.close()\n",
    "\n",
    "file_x_2 = open('Test_features.pickle', 'wb')\n",
    "pickle.dump(Test_features, file_x_2)\n",
    "file_x_2.close()\n",
    "\n",
    "file_x_3 = open('Train_label.pickle', 'wb')\n",
    "pickle.dump(Train_label, file_x_3)\n",
    "file_x_3.close()\n",
    "\n",
    "file_x_4 = open('Test_label.pickle', 'wb')\n",
    "pickle.dump(Test_label, file_x_4)\n",
    "file_x_4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Train_features.pickle', 'rb') as file:\n",
    "    Train_features =pickle.load(file)\n",
    "    \n",
    "with open('Test_features.pickle', 'rb') as file:\n",
    "    Test_features =pickle.load(file)\n",
    "    \n",
    "with open('Train_label.pickle', 'rb') as file:\n",
    "    Train_label =pickle.load(file)\n",
    "    \n",
    "with open('Test_label.pickle', 'rb') as file:\n",
    "    Test_label =pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
