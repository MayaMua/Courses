{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 今天是2020年08月16日，今天世界上又多了一名AI工程师 :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 备注：代码复现见 Reproduce code.ipynb 文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:{    \n",
    "        \"人脸识别技术\":[\"支付宝等人脸支付系统\",\"火车站的人脸识别进站\"],   \n",
    "        \"手机语音助手\":[\"小爱同学\",\"siri\"],  \n",
    "        \"推荐系统应用\":[\"虾米音乐的每日推荐\",\"淘宝购物时的推荐\"],  \n",
    "}  \n",
    "上述均是AI方面的应用，涉及CV图像、自然语言处理、推荐系统等3个方面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: {  \n",
    "github是代码托管平台，且存在很多的开源代码，大部分的论文的源代码都可以找到。可以用来托管自己的代码，进行代码版本的管理。  \n",
    "jupyter一般是适用于交互式编程使用，可以实时查看代码运行结果。  \n",
    "Pycharm是Python的代码开发平台，一般适用于开发大型项目的代码管理。类似于C++的IDE:Vistual Studio 2017  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "Probability Model是指以概率论为基础，采用数学统计方法建立的模型。针对语法这种应用场景，一般很难使用数学理论方法去建立模型，单纯从数学角度去判断句子是不是合理比较难，而基于统计学的方法可以去统计现有合法句子的组成成分，再抽象为数学，再进行概率计算。也就是基于统计学建立的概率模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "使用概率统计模型比较早得应该就是早期的机器翻译系统，较早的机器翻译软件进行中英文互译时，由于技术受限，一般是使用人工翻译之后的文本作为数据集，选择待翻译的中文出现的最多次数的对应英文作为输出。  \n",
    "也可以使用在推荐系统中，在统计某个人喜好的物品时，通常会使用浏览次数，购买次数来计算该人会再次购买该商品的概率，这里也是基于概率统计得出的结果。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "一般来说，使用概率模型的原因主要是因为应用场景中很难通过数学分析去得到模型，基于统计学概率模型得到的模型容易理解，而且在部分情况下可以得到不错的结果。比如在传统的机器翻译模型中，从其他角度去建立翻译模型是很困难的，而基于概率模型进行建模，可以保证有着概率最大化的输出结果。编程难点在于很难去匹配所有的模式，而且一旦模式发生改变，则对应的程序代码就需要针对性的做出调整。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "Language Model：语言模型，一般用于判断一句话或者表示一个序列出现的概率。对于一个好的语言模型来说，给定一句英文或者中文，计算其概率，符合英文语法的，则输出的P概率会高，而不符合语法规则的，输出概率会低。总结来说，Language Model是一个语言里的字符串的概率分布模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "可以使用在需要进行语法判断的场合，比如在OCR文字识别系统中，假设有一句话是:2020年8月19日，其中的数字0因为和字母o有较大的相似，因此OCR并不能准确分辨,但经过语言模型之后字符串\"2020年8月19日\"的概率比\"202o年8月19日\"概率高，因此最终输出2020年8月19日。因此对于语言模型可以用来判断在OCR最终输出结果后的语法上的修正。  \n",
    "同样也可以用在翻译系统中，用于判断最终翻译结果输出是否符合已有的统计语言样本。\n",
    "还有在垃圾邮件分析系统中，也可以用语言模型判断是垃圾邮件的可能有多大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:1-gram也即一元文法模型，这个模型的意思是每个单词出现的概率是独立，跟上下文均没有关系，概率形式为:  \n",
    "$$ Pro(w_1 w_2 w_3 w_4) = Pr(w_1) * P(w_2) * Pr(w_3) * Pr(w_4)$$ \n",
    "即一个句子出现的概率等于每个句子中每个单词概率的乘积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "优点:不考虑上下之间的联系，计算速度等很快，代码开发简单，直观易理解，容易训练相关参数。\n",
    "缺点:因为不考虑上下文之间的联系，只是对数据集单词出现个数进行统计，会出现较大误差，准确性不高。而且单纯的统计频次，泛化能力差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:2-gram模型就是不认为每个单词的出现不再是独立的了，而是跟每个单词前面一个单词有关系，考虑了前文的相关信息。概率形式为:\n",
    "$$ Pro(w_1 w_2 w_3 w_4) = Pr(w_1) * P(w_2|w_1) * Pr(w_3|w_2) * Pr(w_4|w_3)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "\n",
    "> \n",
    ">\n",
    "\n",
    "![WstWorld](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1569578233461&di=4adfa7597fb380e7cc0e67190bbd7605&imgtype=0&src=http%3A%2F%2Fs1.sinaimg.cn%2Flarge%2F006eYYfyzy76cmpG3Yb1f)\n",
    "\n",
    "> \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于去水果店购买水果的语言\n",
    "buyer_grammar = '''\n",
    "buyer = 主语 谓语 动作 数量 形容词* 物品\n",
    "主语 = 我 | 我们 | 俺\n",
    "谓语 = 想 | 需要\n",
    "动作 = 找 | 称 | 买\n",
    "数量 = 几斤 | 几个 | 一些\n",
    "形容词* = null | 形容词 形容词*\n",
    "形容词 = 大大的 | 圆圆的 | 新鲜的\n",
    "物品 = 橙子 | 苹果 | 梨子 | 西瓜\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**： 是否提出了和课程上区别较大的语法结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_grammar = '''\n",
    "seller = 称呼 报数 . 询问 结尾\n",
    "称呼 = 敬语 称谓  : 打招呼 ,\n",
    "敬语 = 尊敬的 | 可爱的\n",
    "称谓 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好\n",
    "报数 = 我是 数字 号导购员\n",
    "数字 = 单个数字 | 数字 单个数字\n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "询问 = 询问1 | 询问2\n",
    "询问1 = 请问 动作 物品类别\n",
    "动作 = 需要买 | 在找\n",
    "物品类别 = 水果 | 蔬菜 | 肉\n",
    "询问2 = 今天 产品 形容词 ,来点\n",
    "产品 = 苹果 | 香蕉 | 青菜\n",
    "形容词 = 很新鲜 | 很便宜 | 在打折\n",
    "结尾 = 吗？\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**：是否和上一个语法区别比较大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义提取语法树的函数\n",
    "import random\n",
    "random.seed(123456)\n",
    "def create_grammar(grammar_str, split='=', line_split='\\n'):   \n",
    "    \"\"\"提取语法树结构，并保存至字典中\"\"\"\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split(line_split):     # 依次处理语法树的每一行\n",
    "        if not line.strip(): continue              # 空行跳过\n",
    "        exp, stmt = line.split(split)\n",
    "        grammar[exp.strip()] = [s.split() for s in stmt.split('|')]\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成购买者和导购者的语法树\n",
    "seller_grammar_create = create_grammar(seller_grammar)\n",
    "buyer_grammar_create = create_grammar(buyer_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(gram, target):\n",
    "    if target not in gram: return target # means target is a terminal expression #1\n",
    "    expaned = [generate(gram, t) for t in random.choice(gram[target])]  #2\n",
    "    return ''.join([e if e != '/n' else '\\n' for e in expaned if e != 'null']) #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展开分析\n",
    "def generate_another(gram, target):\n",
    "    if target not in gram:\n",
    "        return target\n",
    "    expaed = []\n",
    "    for t in random.choice(gram[target]):\n",
    "        expaed.append(generate_another(gram, t))\n",
    "    return ''.join([e if e != '/n' else '\\n' for e in expaed if e != 'null'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'尊敬的先生:你好,我是24号导购员.今天苹果在打折,来点吗？'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(seller_grammar_create, \"seller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'俺需要找几斤西瓜'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(buyer_grammar_create, \"buyer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'尊敬的小朋友:你好,我是2号导购员.请问需要买蔬菜吗？'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_another(seller_grammar_create, \"seller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'俺需要称几个橙子'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_another(buyer_grammar_create, \"buyer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n(gram, target, sentence_number=10):\n",
    "    sentence_list = []\n",
    "    for i in range(sentence_number):\n",
    "        sentence_list.append(generate(gram, target))\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['尊敬的先生:你好,我是18号导购员.请问需要买水果吗？',\n",
       " '可爱的先生:你好,我是672号导购员.请问需要买水果吗？',\n",
       " '可爱的先生:你好,我是775号导购员.请问在找水果吗？',\n",
       " '可爱的先生:您好,我是66号导购员.请问需要买水果吗？',\n",
       " '尊敬的女士:您好,我是1号导购员.今天青菜在打折,来点吗？',\n",
       " '可爱的女士:你好,我是2号导购员.请问需要买水果吗？',\n",
       " '尊敬的小朋友:你好,我是2号导购员.请问在找肉吗？',\n",
       " '可爱的女士:你好,我是651号导购员.今天香蕉很新鲜,来点吗？',\n",
       " '可爱的小朋友:您好,我是2号导购员.请问需要买水果吗？',\n",
       " '尊敬的女士:你好,我是1号导购员.请问需要买蔬菜吗？']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n(seller_grammar_create, \"seller\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我们需要找几个梨子', '俺想买几斤大大的西瓜', '我需要买一些苹果', '我们想找一些橙子', '我想称几个新鲜的新鲜的大大的新鲜的苹果']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n(buyer_grammar_create, \"buyer\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**; 运行代码，观察是否能够生成多个句子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作业部分\n",
    "#### 数据集路径设置及Python包导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数据集进行处理\n",
    "import os\n",
    "dataset_1_name = \"movie_comments.csv\"   # 数据集1的文件名\n",
    "dataset_2_name = \"data_homework1.txt\"\n",
    "dataset_path = os.path.abspath(os.path.join(os.getcwd(), \"./dataset\"))  # 数据集所在路径\n",
    "dataset_1_abs_path = os.path.join(dataset_path, dataset_1_name)\n",
    "dataset_2_abs_path = os.path.join(dataset_path, dataset_2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入工具包\n",
    "import re\n",
    "import random\n",
    "import jieba               # 中文需要分词\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据集1处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ice2019\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>“犯我中华者，虽远必诛”，吴京比这句话还要意淫一百倍。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>脑子是个好东西，希望编剧们都能有。</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>三星半，实打实的7分。第一集在爱国主旋律内部做着各种置换与较劲，但第二集才真正显露吴京的野心...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>开篇长镜头惊险大气引人入胜 结合了水平不俗的快剪下实打实的真刀真枪 让人不禁热血沸腾 特别弹...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>15/100吴京的冷峰在这部里即像成龙，又像杰森斯坦森，但体制外的同类型电影，主角总是代表个...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        link name  \\\n",
       "0   1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1   2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2   3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3   4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4   5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "5   6  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "6   7  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "7   8  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "8   9  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "9  10  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  \n",
       "5                        “犯我中华者，虽远必诛”，吴京比这句话还要意淫一百倍。    1  \n",
       "6                                  脑子是个好东西，希望编剧们都能有。    2  \n",
       "7  三星半，实打实的7分。第一集在爱国主旋律内部做着各种置换与较劲，但第二集才真正显露吴京的野心...    4  \n",
       "8  开篇长镜头惊险大气引人入胜 结合了水平不俗的快剪下实打实的真刀真枪 让人不禁热血沸腾 特别弹...    4  \n",
       "9  15/100吴京的冷峰在这部里即像成龙，又像杰森斯坦森，但体制外的同类型电影，主角总是代表个...    1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1_src = pd.read_csv(dataset_1_abs_path, encoding='utf-8')   # 记载数据集\n",
    "data_1_src.head(10)   # 查看数据集情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1_articles = data_1_src[\"comment\"].tolist() # 提取数据集中\"content\"内容\n",
    "len(data_1_articles)      # 输出数据集的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义从字符串中提取文本的函数\n",
    "def token(string):\n",
    "    # 匹配单词字符，即a-z,A-Z,0-9,_\n",
    "    return re.findall(\"\\w+\", string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据集1处理完成保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "血战钢锯岭中国人也会觉得好看因为它歌颂的宗教情怀是超越政权的但当你只想歌颂一个政权时很明显就低了一个层次甚至充满了现实乃至投机的考量高下立见\n"
     ]
    }
   ],
   "source": [
    "data_1_articles_clean = [''.join(token(str(a)))for a in data_1_articles]   # 遍历文章内容,token函数实现单词字符的提取\n",
    "# 保存处理好之后的文本\n",
    "data_1_articles_clean_path = os.path.join(dataset_path, 'data_1_articles_clean.txt')\n",
    "with open(data_1_articles_clean_path, 'w',encoding=\"utf-8\") as f:\n",
    "    for a in data_1_articles_clean:\n",
    "        f.write(a + '\\n')\n",
    "print(data_1_articles_clean[34])   # 输出结果内容查看"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 针对数据集2进行处理-保险数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 首先加载数据集观察数据集情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0 ++$++ disability-insurance ++$++ 法律要求残疾保险吗？ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 ++$++ life-insurance ++$++ 债权人可以在死后人寿保险吗？ ++...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 ++$++ renters-insurance ++$++ 旅行者保险有租赁保险吗？ +...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3 ++$++ auto-insurance ++$++ 我可以开一辆没有保险的新车吗？ +...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4 ++$++ life-insurance ++$++ 人寿保险的现金转出价值是否应纳税？...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5 ++$++ annuities ++$++ 如何报告年金收入？ ++$++ How  I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6 ++$++ home-insurance ++$++ AAA家庭保险涵盖什么？ ++$+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7 ++$++ retirement-plans ++$++ 什么是简单的退休计划？ ++$...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8 ++$++ disability-insurance ++$++ 社会保险残疾保险是什么...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9 ++$++ auto-insurance ++$++ 汽车保险是否预付？ ++$++ I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment\n",
       "0  0 ++$++ disability-insurance ++$++ 法律要求残疾保险吗？ ...\n",
       "1  1 ++$++ life-insurance ++$++ 债权人可以在死后人寿保险吗？ ++...\n",
       "2  2 ++$++ renters-insurance ++$++ 旅行者保险有租赁保险吗？ +...\n",
       "3  3 ++$++ auto-insurance ++$++ 我可以开一辆没有保险的新车吗？ +...\n",
       "4  4 ++$++ life-insurance ++$++ 人寿保险的现金转出价值是否应纳税？...\n",
       "5  5 ++$++ annuities ++$++ 如何报告年金收入？ ++$++ How  I...\n",
       "6  6 ++$++ home-insurance ++$++ AAA家庭保险涵盖什么？ ++$+...\n",
       "7  7 ++$++ retirement-plans ++$++ 什么是简单的退休计划？ ++$...\n",
       "8  8 ++$++ disability-insurance ++$++ 社会保险残疾保险是什么...\n",
       "9  9 ++$++ auto-insurance ++$++ 汽车保险是否预付？ ++$++ I..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2_src = pd.read_csv(dataset_2_abs_path, encoding = 'utf-8',header =None ,delimiter='\\t', names=['comment'])   # 记载数据集\n",
    "data_2_src.head(10)   # 查看数据集情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12889"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2_articles = data_2_src[\"comment\"].tolist() # 提取数据集中\"content\"内容\n",
    "len(data_2_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义只提取中文的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_chinese(string):\n",
    "    # 匹配单词字符，即这里因为只涉及到中文处理，因此这里只提取中文\n",
    "    return re.findall(u\"[\\u4e00-\\u9fa5]+\", string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对数据集2提取所有的中文文本并进行保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可以在乳腺癌后获得人身保险吗\n"
     ]
    }
   ],
   "source": [
    "# 观察到数据集中存在很多非中文的文本,这里是对中文句子进行分析,删除过多的英文\n",
    "data_2_articles_clean = [''.join(token_chinese(str(a)))for a in data_2_articles]   # 遍历文章内容,token函数实现单词字符的提取\n",
    "# 保存处理好之后的文本\n",
    "data_2_articles_clean_path = os.path.join(dataset_path, 'data_2_articles_clean.txt')\n",
    "with open(data_2_articles_clean_path, 'w',encoding=\"utf-8\") as f:\n",
    "    for a in data_2_articles_clean:\n",
    "        f.write(a + '\\n')\n",
    "print(data_2_articles_clean[110])   # 输出结果内容查看"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义中文分词函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string): \n",
    "    return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = []    # 存放分词之后的中文单词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载数据集1的内容，并将分词后的数据保存至列表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ice2019\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.775 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate((open(data_1_articles_clean_path, encoding=\"utf-8\"))):\n",
    "    if i % 50000 == 0:  # 每5000次输出一次\n",
    "        print(i) \n",
    "    TOKEN += cut(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载数据集2的内容，并将分词后的数据保存至列表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate((open(data_2_articles_clean_path, encoding=\"utf-8\"))):\n",
    "    if i % 5000 == 0:  # 每5000次输出一次\n",
    "        print(i) \n",
    "    TOKEN += cut(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吴京', '意淫', '到', '了', '脑残', '的', '地步', '看', '了', '恶心', '想', '吐', '\\n', '首映礼', '看', '的', '太', '恐怖', '了', '这个']\n"
     ]
    }
   ],
   "source": [
    "print(TOKEN[0:20])    # 查看数据结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 331482),\n",
       " ('\\n', 274386),\n",
       " ('了', 102558),\n",
       " ('是', 75450),\n",
       " ('我', 52389),\n",
       " ('都', 36283),\n",
       " ('很', 34718),\n",
       " ('看', 34027),\n",
       " ('电影', 33675),\n",
       " ('在', 32158)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此时已经所有的文章进行了分词，并保存在了TOKEN列表中\n",
    "words_count = Counter(TOKEN)  # 单词统计\n",
    "words_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1da2f878b70>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "frequiences = [f for w, f in words_count.most_common(100)]   # 计算出频率\n",
    "x = [i for i in range(100)]\n",
    "plt.plot(x, frequiences)           # 绘制次数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 统计文本数据中各个单词的出现的频率-视作概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_number = len(TOKEN)    # 统计所有的文本数量\n",
    "def prob_1(word):\n",
    "    return words_count[word] / words_number   # 返回word的统计频率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0012258713409590025"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_1('我们')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吴京意淫', '意淫到', '到了', '了脑残', '脑残的', '的地步', '地步看', '看了', '了恶心', '恶心想']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN = [str(t) for t in TOKEN]\n",
    "TOKEN_2_GRAM = [''.join(TOKEN[i:i+2]) for i in range(len(TOKEN[:-2]))]\n",
    "TOKEN_2_GRAM[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count_2 = Counter(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算概率 \n",
    "def prob_2(word1, word2):  # p(w1,w2) = count(w1,2)/count(w1)\n",
    "    if word1 + word2 in words_count_2: \n",
    "        return words_count_2[word1+word2] / words_count[word1]\n",
    "    else:\n",
    "        return 1 / len(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 说明:  这里计算的是如下的概率:\n",
    "##### 如: “我们”  “在” “吃饭”\n",
    "##### 这里计算的是P(w2|w1) = P(w1+w2)/P(w1)  也就是在单词1出现的情况下，单词2出现的概率，前后的顺序不能变，即我们在前   在是第二个词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016351989211058664"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_2('我们', '在')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0029230673549350086"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_2('在', '我们')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不计算P(w1)\n",
    "def get_probablity(sentence):\n",
    "    words = cut(sentence)\n",
    "    sentence_pro = 1\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        next_ = words[i+1]\n",
    "        probability = prob_2(word, next_)  # p(w1|w2)\n",
    "        sentence_pro *= probability  # p(s) = p(w_1)p(w2|w1)*p(w3|w2)..p(wn|wn-1) \n",
    "#         if i == 0:\n",
    "#             sentence_pro *= prob_1(word)\n",
    "    return sentence_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同时计算P(w1)\n",
    "def get_probablity_first(sentence):\n",
    "    words = cut(sentence)\n",
    "    sentence_pro = 1\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        next_ = words[i+1]\n",
    "        probability = prob_2(word, next_)  # p(w1|w2)\n",
    "        sentence_pro *= probability  # p(s) = p(w_1)p(w2|w1)*p(w3|w2)..p(wn|wn-1) \n",
    "        if i == 0:\n",
    "            sentence_pro *= prob_1(word)\n",
    "    return sentence_pro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算首个单词的概率之后，输出句子的数据集概率变得很小,而且一般来说首字的概率对句子的整体概率影响应该不能占较大比重的，因为我们考虑的是句子的语法是不是合法，而每个句子的首单词如果对句子概率产生较大影响则会影响对句子合法性的判断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1660595761316844e-35"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity('小明今天抽奖抽到一台苹果手机')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.788674866980156e-41"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity('小明今天抽奖抽到一架波音飞机')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点** 1. 是否使用了新的数据集； 2. csv(txt)数据是否正确解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1, 3, 5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数接受一个参数key，这个参数接受一个函数作为输入，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第0个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0), (1, 4), (4, 4), (2, 5)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (1, 4), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序, 但是是递减的顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(gram, target, get_probablity, sentence_number=20): # you code here\n",
    "    sentence_list = generate_n(gram=gram, target=target, sentence_number=sentence_number)\n",
    "    sentence_probablity_list = []\n",
    "    for i, sentence in enumerate(sentence_list):\n",
    "        sentence_probablity_list.append((sentence, get_probablity(sentence)))\n",
    "    print(\"生成的句子和概率分别是:\")\n",
    "    for sentence, prob in sentence_probablity_list:\n",
    "        print(sentence, \"\\t\", prob)\n",
    "    sentence_probablity_list_sorted = sorted(sentence_probablity_list, key=lambda x:x[1], reverse=True)\n",
    "    return sentence_probablity_list_sorted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的句子和概率分别是:\n",
      "尊敬的先生:您好,我是2号导购员.今天苹果很便宜,来点吗？ \t 1.240494452865056e-105\n",
      "尊敬的先生:您好,我是7号导购员.请问需要买蔬菜吗？ \t 4.933718738469661e-95\n",
      "尊敬的小朋友:您好,我是5号导购员.今天香蕉很便宜,来点吗？ \t 9.785175306006169e-104\n",
      "可爱的小朋友:你好,我是1号导购员.请问在找肉吗？ \t 2.4070804567518992e-84\n",
      "尊敬的小朋友:您好,我是6号导购员.请问需要买肉吗？ \t 3.757520191218494e-91\n",
      "可爱的先生:您好,我是65号导购员.请问需要买水果吗？ \t 5.343630705332626e-102\n",
      "尊敬的女士:您好,我是1号导购员.今天青菜很便宜,来点吗？ \t 6.286041754142396e-105\n",
      "尊敬的女士:您好,我是4号导购员.请问在找肉吗？ \t 3.0290022978229662e-86\n",
      "可爱的小朋友:您好,我是687948号导购员.今天苹果很新鲜,来点吗？ \t 3.406552903353207e-110\n",
      "尊敬的小朋友:您好,我是1号导购员.今天苹果很新鲜,来点吗？ \t 1.1220584531144182e-102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('可爱的小朋友:你好,我是1号导购员.请问在找肉吗？', 2.4070804567518992e-84)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_best(seller_grammar_create, \"seller\", get_probablity, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**： 是否使用 lambda 语法进行排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:{  \n",
    "1.首先为了能够学到足够多的正常语法，肯定希望是增加数据集，越多的真实数据样本训练出来的结果可能会更好。但是增加过多的数据集可能会导致内存不够、处理速度慢等问题。  \n",
    "2.其实如果在最后处理的数据中出现了未在训练集中出现的字词，这些词就是Out-of-vocabulary，简称OOV。这些字词就无法进行准确处理。  \n",
    "3.这里使用的是2-gram模型，也即当前字出现的概率只跟前面一个字符有关系，但是实际情况中当前位置出现的字词的概率可能与较远位置的字词有关，比如\"可爱的先生:您好,我是65号导购员.请问需要买水果吗？\",这里的\"先生\"与\"可爱\"之间相隔就超过了2个字符，可以通过使用N-gram模型来解决，但是增加的N会使资源消耗快速增加。使用3-gram或者4-gram相对好些。  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**评阅点**: 是否提出了比较实际的问题，例如OOV问题，例如数据量，例如变成 3-gram问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下内容为可选部分，对于绝大多数同学，能完成以上的项目已经很优秀了，下边的内容如果你还有精力可以试试，但不是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 另外一份作业文件里有个optional，有兴趣的同学可以挑战一下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
